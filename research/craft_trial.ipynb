{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\GCET\\\\Machine Learning\\\\Deep Learning Projects\\\\text_recognition\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\GCET\\\\Machine Learning\\\\Deep Learning Projects\\\\text_recognition'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRAFT Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TextDetectionConfig:\n",
    "    craft_weights: Path\n",
    "    refiner_weights: Path\n",
    "    loaded_image_path: Path\n",
    "    normalized_image_path: Path\n",
    "    resized_data_path: Path\n",
    "    text_threshold: float\n",
    "    low_text: float\n",
    "    link_threshold: float\n",
    "    poly: bool\n",
    "    refine: bool\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuration\n",
    "from mlOCR.constants import *\n",
    "from mlOCR.utils.common import read_yaml,create_directories\n",
    "from pathlib import Path\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_text_detection_config(self)->TextDetectionConfig:\n",
    "        config=self.config.text_detection\n",
    "        params=self.params.text_detection\n",
    "\n",
    "        text_detection_config=TextDetectionConfig(\n",
    "            craft_weights=Path(config.craft_weights),\n",
    "            refiner_weights=Path(config.refiner_weights),\n",
    "            loaded_image_path=Path(config.loaded_image_path),\n",
    "            normalized_image_path=Path(config.normalized_image_path),\n",
    "            resized_data_path=Path(config.resized_data_path),\n",
    "            text_threshold=float(params.text_threshold),\n",
    "            low_text=float(params.low_text),\n",
    "            link_threshold=float(params.link_threshold),\n",
    "            poly=bool(params.poly),\n",
    "            refine=bool(params.refine)\n",
    "        )\n",
    "\n",
    "        return text_detection_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#component\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "from skimage import io\n",
    "from mlOCR.components.Image_Processing import ImageProcessing\n",
    "from mlOCR.models.craft import CRAFT\n",
    "from mlOCR.models.refine_net import RefineNet\n",
    "from mlOCR.utils.common import *\n",
    "from mlOCR.utils.text_detection_utils import *\n",
    "from mlOCR import logger\n",
    "\n",
    "\n",
    "class TextDetection:\n",
    "    def __init__(self,TextDetectionConfig):\n",
    "        self.craft_weights=TextDetectionConfig.craft_weights\n",
    "        self.refiner_weights=TextDetectionConfig.refiner_weights\n",
    "        self.loaded_image_path=TextDetectionConfig.loaded_image_path\n",
    "        self.normalized_image_path=TextDetectionConfig.normalized_image_path\n",
    "        self.resized_data_path=TextDetectionConfig.resized_data_path\n",
    "        self.text_threshold=TextDetectionConfig.text_threshold\n",
    "        self.low_text=TextDetectionConfig.low_text\n",
    "        self.link_threshold=TextDetectionConfig.link_threshold\n",
    "        self.poly=TextDetectionConfig.poly\n",
    "        self.refine=TextDetectionConfig.refine\n",
    "\n",
    "    def copyStateDict(self,state_dict):\n",
    "        if list(state_dict.keys())[0].startswith(\"module\"):\n",
    "            start_idx = 1\n",
    "        else:\n",
    "            start_idx = 0\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in state_dict.items():\n",
    "            name = \".\".join(k.split(\".\")[start_idx:])\n",
    "            new_state_dict[name] = v\n",
    "        return new_state_dict\n",
    "    \n",
    "    def modelInference(self,norm_image):\n",
    "        net=CRAFT()\n",
    "        net.load_state_dict(self.copyStateDict(torch.load(self.craft_weights, map_location='cpu')))\n",
    "        logger.info(f'Craft model weights loaded successfully!')\n",
    "        net.eval()\n",
    "\n",
    "        x=norm_image\n",
    "        x=x.astype('float32')\n",
    "        x = torch.from_numpy(x).permute(2, 0, 1)    # [h, w, c] to [c, h, w]\n",
    "        x = Variable(x.unsqueeze(0))                # [c, h, w] to [b, c, h, w]\n",
    "\n",
    "        # forward pass\n",
    "        with torch.no_grad():\n",
    "            y, feature = net(x)\n",
    "\n",
    "        # make score and link map\n",
    "        score_text = y[0,:,:,0].cpu().data.numpy()\n",
    "        score_link = y[0,:,:,1].cpu().data.numpy()\n",
    "\n",
    "        if self.refine:\n",
    "            refine_net=RefineNet()\n",
    "            refine_net.load_state_dict(self.copyStateDict(torch.load(self.refiner_weights, map_location='cpu')))\n",
    "            logger.info(f'RefineNet model weights loaded successfully!')\n",
    "            refine_net.eval()\n",
    "            self.poly=True\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_refiner = refine_net(y, feature)\n",
    "            score_link = y_refiner[0,:,:,0].cpu().data.numpy()\n",
    "\n",
    "        resized_data = load_bin(self.resized_data_path)\n",
    "        ratio, size_heatmap = resized_data[\"ratio\"], resized_data[\"size_heatmap\"]\n",
    "        ratio_h = ratio_w = 1 / ratio\n",
    "        \n",
    "        # Post-processing\n",
    "        boxes, polys = getDetBoxes(score_text, score_link, self.text_threshold, self.link_threshold, self.low_text, self.poly)\n",
    "\n",
    "        # coordinate adjustment\n",
    "        boxes = adjustResultCoordinates(boxes, ratio_w, ratio_h)\n",
    "        polys = adjustResultCoordinates(polys, ratio_w, ratio_h)\n",
    "        for k in range(len(polys)):\n",
    "            if polys[k] is None: polys[k] = boxes[k]\n",
    "\n",
    "        render_img = score_text.copy()\n",
    "        render_img = np.hstack((render_img, score_link))\n",
    "        ret_score_text = cvt2HeatmapImg(render_img)\n",
    "\n",
    "        return boxes, polys, ret_score_text\n",
    "    \n",
    "    def generate_result(self):\n",
    "        image=io.imread(self.loaded_image_path)\n",
    "        norm_image=io.imread(self.normalized_image_path)\n",
    "\n",
    "        boxes, polys, ret_score_text=self.modelInference(norm_image)\n",
    "\n",
    "        filename, file_ext = os.path.splitext(os.path.basename(self.loaded_image_path))\n",
    "        mask_image='artifacts/image/result/res_'+filename+'_mask.jpg'\n",
    "        io.imsave(mask_image,ret_score_text)\n",
    "\n",
    "        saveResult(self.loaded_image_path, image[:,:,::-1], polys, dirname='artifacts/image/result/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-04 12:41:08,303: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-03-04 12:41:08,307: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-03-04 12:41:08,309: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-03-04 12:41:08,310: INFO: common: created directory at: artifacts]\n",
      "[2025-03-04 12:41:10,320: INFO: 1655308015: Craft model weights loaded successfully!]\n",
      "[2025-03-04 12:41:13,008: INFO: common: binary file loaded from: artifacts\\image\\result\\resized_data.pkl]\n"
     ]
    }
   ],
   "source": [
    "#pipeline\n",
    "\n",
    "try:\n",
    "    config=ConfigurationManager()\n",
    "    text_detection_config=config.get_text_detection_config()\n",
    "    text_detection=TextDetection(text_detection_config)\n",
    "    text_detection.generate_result()\n",
    "except Exception as e:\n",
    "    logger.exception(e)\n",
    "    raise e\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vir_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
